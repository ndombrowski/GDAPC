[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GDA: Preparing data",
    "section": "",
    "text": "As you continue to build on your understanding of the topics from the first two courses, you’ll also be introduced to new topics that will help you gain practical data analytics skills. You’ll learn how to use tools like spreadsheets and SQL to extract and make use of the right data for your objectives and how to organize and protect your data. Current Google data analysts will continue to instruct and provide you with hands-on ways to accomplish common data analyst tasks with the best tools and resources.\nCourse objectives\n\nFind out how analysts decide which data to collect for analysis.\nLearn about structured and unstructured data, data types, and data formats.\nDiscover how to identify different types of bias in data to help ensure data credibility.\nExplore how analysts use spreadsheets and SQL with databases and data sets.\nExamine open data and the relationship between and importance of data ethics and data privacy.\nGain an understanding of how to access databases and extract, filter, and sort the data they contain.\nLearn the best practices for organizing data and keeping it secure."
  },
  {
    "objectID": "code/1_data_types_and_structures.html",
    "href": "code/1_data_types_and_structures.html",
    "title": "2  Data types and structures",
    "section": "",
    "text": "Data collection considerations:\n\nHow the data will be collected\nChoose data sources\nDecide what data to use\nHow much data to collect\nSelect the right data type\nDetermine the timeframe for data collection, If you need an immediate answer, you might not have time to collect new data. In this case, you would need to use historical data that already exists.\n\nData sources:\n\nFirst-party data: data collected by yourself, an individual or group using their own resources; typically the preferred method because we know where the data came from. For example data from an interview you conducted\nSecond-party data: data collected by a group directly from its audience and then sold. I.e. demographic data collected by a university or census data gathered by the government\nThird-party data: data collected from outside sources who did not collect it directly and might come from a number of different sources\n\nData collection:\n\nA population refers to all possible data values in a certain data set, which can be pretty challenging\nA sample is a part of a population that is representative of this population"
  },
  {
    "objectID": "code/1_data_types_and_structures.html#data-formats-and-structures",
    "href": "code/1_data_types_and_structures.html#data-formats-and-structures",
    "title": "2  Data types and structures",
    "section": "2.2 Data formats and structures",
    "text": "2.2 Data formats and structures\n\n2.2.1 Basic terminology\n\nContinuous data is measured and can have almost any numeric value, i.e. the running time of a movie\nDiscrete data: Data that’s counted and has a limited number of values. For example number of people who visit a hospital on a daily basis (10, 20, 200) or the rooms max. allowed capacity or sold tickets per month\nNominal data: A type of qualitative data that is organized without a set order; i.e. yes, no, or unsure\nOrdinal data: A type of qualitative data with a set order or scale; i.e. if we ask a group of people to rank a movie from 1-5\nInternal data: Data that lives within a companies own system, also called primary data. I.e. Wages of employees tracked by HR or product inventory levels\nExternal data: data that lives and is generated outside of an organization, also called secondary data. I.e. National average wages for the various positions throughout your organization\nStructured data: data that’s organized in a certain format, such as rows and columns. Spreadsheets and relational databases are two examples of software that store data in a structured way. Structured data that is grouped together to form relations enables analysts to more easily store, search, and analyze the data.\nUnstructured data: data that is not organized in any easily identifiable manner, i.e. audio or video files\n\n\n\n2.2.2 Structured data\nStructured data works well in a data model, i.e. a model that is used for organizing data elements and how they relate to each other. Data elements are pieces of information, such as people’s names or account numbers or addresses.\n\n\n2.2.3 Data modeling levels and techniques\nData modeling is the process of creating diagrams that visually represent how data is organized and structured. These visual representations are called data models.\nDifferent levels of data modeling have a different level of detail:\n\nConceptual data modeling gives a high-level view of the data structure, such as how data interacts across an organization. For example, a conceptual data model may be used to define the business requirements for a new database. A conceptual data model doesn’t contain technical details.\nLogical data modeling focuses on the technical details of a database such as relationships, attributes, and entities. For example, a logical data model defines how individual records are uniquely identified in a database. But it doesn’t spell out actual names of database tables. That’s the job of a physical data model.\nPhysical data modeling depicts how a database operates. A physical data model defines all entities and attributes used; for example, it includes table names, column names, and data types for the database.\n\nMore information can be found in this comparison of data models​.\nThere are a lot of approaches when it comes to developing data models, but two common methods are the Entity Relationship Diagram (ERD) and the Unified Modeling Language (UML) diagram. ERDs are a visual way to understand the relationship between entities in the data model. UML diagrams are very detailed diagrams that describe the structure of a system by showing the system’s entities, attributes, operations, and their relationships.\nYou can read more about ERD, UML, and data dictionaries in this data modeling techniques article."
  },
  {
    "objectID": "code/1_data_types_and_structures.html#data-types-field-and-values",
    "href": "code/1_data_types_and_structures.html#data-types-field-and-values",
    "title": "2  Data types and structures",
    "section": "2.3 Data types, field and values",
    "text": "2.3 Data types, field and values\n\nData type: A specific kind of data attribute that tells what kind of value the data is. Data types can be different depending on the query language you’re using\n\n\n2.3.1 Data types in spreadsheets\n\nNumber\nText or string: A sequence of characters and punctuation that contains textual information\nBoolean: A data type with only two possible values, such as True and False\n\n\n\n2.3.2 Boolean logic\nConditions are created with Boolean operators, including AND, OR, and NOT. These operators are similar to mathematical operators and can be used to create logical statements that filter your results.\nImagine you are shopping for shoes, and are considering certain preferences:\n\nYou will buy the shoes only if they are pink and grey using IF (Color=\"Grey\") AND (Color=\"\"\"Pink\")\nYou will buy the shoes if they are entirely pink or entirely grey, or if they are pink and grey using IF (Color=\"Grey\") OR (Color=\"Pink\")\nYou will buy the shoes if they are grey, but not if they have any pink using IF (Color=\"Grey\") AND (Color=NOT “Pink\")\n\n\n\nLearn about who pioneered Boolean logic in this historical article: Origins of Boolean Algebra in the Logic of Classes.\nFind more information about using AND, OR, and NOT from these tips for searching with Boolean operators.\n\n\n\n2.3.3 Data table components\n\nRow - Record\nColumn - Field\n\n\n\n2.3.4 Wide and long data\n\nWide data: every data subject has a single row with multiple columns to hold the values of various attributes of the subject. Wide data lets you easily identify and quickly compare different columns. Wide data is preferred when:\n\nCreating tables and charts with a few variables about each subject\nComparing straightforward line graphs\n\nLong data: data in which each row is one time point per subject, so each subject will have data in multiple rows. Long data is a great format for storing and organizing data when there’s multiple variables for each subject at each time point that we want to observe. With this long data format, we can store and analyze all of this data using fewer columns. This is the preferred format when:\n\nStoring a lot of variables about each subject. For example, 60 years worth of interest rates for each bank\nPerforming advanced statistical analysis or graphing\n\n\n\n\n2.3.5 Data transformation\n= the process of changing the data’s format, structure, or values.\nData transformation usually involves:\n\nAdding, copying, or replicating data\nDeleting fields or records\nStandardizing the names of variables\nRenaming, moving, or combining columns in a database\nJoining one set of data with another\nSaving a file in a different format. For example, saving a spreadsheet as a comma separated values (CSV) file.\n\nGoals for data transformation might be:\n\nData organization: better organized data is easier to use\nData compatibility: different applications or systems can then use the same data\nData migration: data with matching formats can be moved from one system to another\nData merging: data with the same organization can be merged together\nData enhancement: data can be displayed with more detailed fields\nData comparison: apples-to-apples comparisons of the data can then be made"
  },
  {
    "objectID": "code/2_bias_ethics.html",
    "href": "code/2_bias_ethics.html",
    "title": "3  Bias, credibility, privacy, ethics and access",
    "section": "",
    "text": "Bias: A preference in favor of or against a person, group of people, or thing\nData bias: A type of error that systematically skews results in a certain direction\n\nTypes of data bias:\n\nSampling bias: When a sample is not representative of the population as a whole, avoid this by choosing a sample at random\nObserver bias: Sometimes referred to as experimenter bias or research bias. It is the tendency for different people to observe things differently\nInterpretation bias: The tendency to always interpret ambiguous situations in a positive, or negative way\nConfirmation bias: The tendency to search for, or interpret information in a way that confirms preexisting beliefs"
  },
  {
    "objectID": "code/2_bias_ethics.html#identifying-good-data-sources",
    "href": "code/2_bias_ethics.html#identifying-good-data-sources",
    "title": "3  Bias, credibility, privacy, ethics and access",
    "section": "3.2 Identifying good data sources",
    "text": "3.2 Identifying good data sources\n\nReliable: Data that is based on accurate, complete and unbiased information and that’s been vetted and proven fit for use\nOriginal: When dealing with data through a second or third party source, be sure to validate it with the original source\nComprehensive: The data contains all critical information needed\nCurrent: The usefulness of data decreases as time passes\nCited: Citing makes the information you’re providing more credible: Who created the data set? Is it part of a credible organization? When was the data last refreshed?"
  },
  {
    "objectID": "code/2_bias_ethics.html#data-ethics-and-privacy",
    "href": "code/2_bias_ethics.html#data-ethics-and-privacy",
    "title": "3  Bias, credibility, privacy, ethics and access",
    "section": "3.3 Data ethics and privacy",
    "text": "3.3 Data ethics and privacy\n\nEthics: well-founded standards of right and wrong that prescribe what humans ought to do, usually in terms of rights, obligations, benefits to society, fairness or specific virtues\nData ethics: well- founded standards of right and wrong that dictate how data is collected, shared, and used\n\n\n3.3.1 Aspects of data ethics:\n\nOwnership: This answers the question who owns data? I.e. Individuals own the raw data they provide, and they have primary control over its usage, how it’s processed and how it’s shared.\nTransaction transparency: All data processing activities and algorithms should be completely explainable and understood by the individual who provides their data. This is in response to concerns over data bias\nConsent: An individual’s right to know explicit details about how and why their data will be used before agreeing to provide it\nCurrency: Individuals should be aware of financial transactions resulting from the use of their personal data and the scale of these transactions\nPrivacy: Preserving a data subject’s information and activity any time a data transaction occurs, i.e. information privacy or data protection. It also covers a person’s legal right to their data:\n\nProtection from unauthorized access to our private data,\nFreedom from inappropriate use of our data,\nThe right to inspect, update, or correct our data,\nThe ability to give consent to use our data,\nThe legal right to access our data\n\nOpenness: Free access, usage, and sharing of data\n\n\n\n3.3.2 Data anonymization\nData anonymization is the process of protecting people’s private or sensitive data by eliminating that kind of information. Typically, data anonymization involves blanking, hashing, or masking personal information, often by using fixed-length codes to represent data columns, or hiding data with altered values.\nHealthcare and financial data are two of the most sensitive types of data. That’s why data in these two industries usually goes through de-identification, which is a process used to wipe data clean of all personally identifying information."
  },
  {
    "objectID": "code/2_bias_ethics.html#open-data",
    "href": "code/2_bias_ethics.html#open-data",
    "title": "3  Bias, credibility, privacy, ethics and access",
    "section": "3.4 Open data",
    "text": "3.4 Open data\n\nOpen data must be available as a whole, preferably by downloading over the Internet in a convenient and modifiable form\nOpen data must be provided under terms that allow reuse and redistribution including the ability to use it with other datasets\nEveryone must be able to use, reuse, and redistribute the data, i.e. universal participation  should be possible\nInteroperability is key to open data’s success. Interoperability is the ability of data systems and services to openly connect and share data\n\nOne of the biggest benefits of open data is that credible databases can be used more widely. Basically, this means that all of that good data can be leveraged, shared, and combined with other data. But it is important to think about the individuals being represented by the public, open data, too.\nPersonal identifiable information (PII)  is data that is reasonably likely to identify a person and make information known about them. It is important to keep this data safe. PII can include a person’s address, credit card information, social security number, medical records, and more.\nResources:\n\nU.S. government data site: Data.gov is one of the most comprehensive data sources in the US. This resource gives users the data and tools that they need to do research, and even helps them develop web and mobile applications and design data visualizations.\nU.S. Census Bureau: This open data source offers demographic information from federal, state, and local governments, and commercial entities in the U.S. too.\nOpen Data Network: This data source has a really powerful search engine and advanced filters. Here, you can find data on topics like finance, public safety, infrastructure, and housing and development.\nGoogle Cloud Public Datasets: There are a selection of public datasets available through the Google Cloud Public Dataset Program that you can find already loaded into BigQuery.\n\nDataset Search: The Dataset Search is a search engine designed specifically for data sets; you can use this to search for specific data sets."
  },
  {
    "objectID": "code/3_working_with_databases.html",
    "href": "code/3_working_with_databases.html",
    "title": "4  Working with databases",
    "section": "",
    "text": "A database is a collection of data stored in a computer system.\nRelational database: A database that contains a series of related tables that can be connected via their relationships. They allow data analysts to organize and link data based on what the data has in common. For two tables to have a relationship, one or more of the same fields must exist inside both tables. Tables in a relational database are connected by the fields they have in common.\nNormalization is a process of organizing data in a relational database. For example, creating tables and establishing relationships between those tables. It is applied to eliminate data redundancy, increase data integrity, and reduce complexity in a database.\nPrimary key: A identifier that references a column in which each value is unique. n other words, it’s a column of a table that is used to uniquely identify each record within that table. If you do decide to include a primary key, it should be unique, meaning no two rows can have the same primary key. Also, it cannot be null or blank.\nForeign key: A field within a table that is a primary key in another table. In other words, a foreign key is how one table can be connected to another. A table can have only one primary but multiple foreign keys. These keys are what create the relationships between tables in a relational database, which helps organize and connect data across multiple tables in the database.\nSome tables don’t require a primary key\nBoth primary and foreign keys connect tables in relational databases\nA primary key may also be constructed using multiple columns of a table. This type of primary key is called a composite key"
  },
  {
    "objectID": "code/3_working_with_databases.html#managing-data-with-metadata",
    "href": "code/3_working_with_databases.html#managing-data-with-metadata",
    "title": "4  Working with databases",
    "section": "4.2 Managing data with metadata",
    "text": "4.2 Managing data with metadata\n\nMetadata: Data about data but not data itself. Metadata tells you where the data comes from, when and how it was created, and what it’s all about. Metadata helps data analysts interpret the contents of the data within a database. Metadata also makes data more reliable by making sure it’s accurate, precise, relevant, and timely.\nMetadata helps data analysts interpret the contents of the data within a database\nExamples of parts of metadata: Title and description, tags and categories, who created it and when it was created, who last modified it and when, who can access or update it?\nA metadata repository is a database specifically created to store metadata.These repositories describe where metadata came from, keep it in an accessible form so it can be used quickly and easily, and keep it in a common structure for everyone who may need to use it. They:\n\nDescribe the state and location of metadata\nDescribe the structures of the tables inside\nDescribe how the data flows through the repository\nKeeps track of who accesses the metadata and when\n\n\nTypes of metadata:\n\nDescriptive: Metadata that describes a piece of data and can be used to identify it at a later point in time, i.e. ID numbers of students\nStructural: Metadata that indicates how a piece of data is organized and whether it is part of one, or more than one, data collection\nAdministrative: Metadata that indicates the technical source and details of a digital asset, i.e. The date and time a photo was taken\n\nManaging metadata:\n\nMetadata is stored in a single, central location and it gives the company standardized information about all of its data. This is done in two ways:\n\nMetadata includes information about where each system is located and where the data sets are located within those systems\nThe metadata describes how all of the data is connected between the various systems\n\nData governance is a process to ensure the formal management of a company’s data assets. This gives an organization better control of their data and helps a company manage issues related to data security and privacy, integrity, usability, and internal and external data flows."
  },
  {
    "objectID": "code/3_working_with_databases.html#accessing-data-sources",
    "href": "code/3_working_with_databases.html#accessing-data-sources",
    "title": "4  Working with databases",
    "section": "4.3 Accessing data sources",
    "text": "4.3 Accessing data sources\n\n4.3.1 Data import\nGoogle Sheets\nIn Google Sheets, you can use the IMPORTRANGE function. It enables you to specify a range of cells in the other spreadsheet to duplicate in the spreadsheet you are working in. You must allow access to the spreadsheet containing the data the first time you import the data.\n\n\n4.3.2 Import HTML tables from the web\nImporting HTML tables is a very basic method to extract or “scrape” data from public web pages. Web scraping made easy introduces how to do this with Google Sheets or Microsoft Excel.\nGoogle Sheets\nIn Google Sheets, you can use the IMPORTHTML function. It enables you to import the data from an HTML table (or list) on a web page.\nExcel\n\nStep 1: Open a new or existing spreadsheet.\nStep 2: Click Data in the main menu and select the From Web option.\nStep 3: Enter the URL and click OK.\nStep 4: In the Navigator, select which table to import.\nStep 5: Click Load to load the data from the table into your spreadsheet."
  },
  {
    "objectID": "code/3_working_with_databases.html#sorting-and-filtering",
    "href": "code/3_working_with_databases.html#sorting-and-filtering",
    "title": "4  Working with databases",
    "section": "4.4 Sorting and filtering",
    "text": "4.4 Sorting and filtering\n\nSorting involves arranging data into a meaningful order to make it easier to understand, analyze, and visualize\nFiltering means showing only the data that meets a specific criteria while hiding the rest"
  },
  {
    "objectID": "code/3_working_with_databases.html#working-with-sql",
    "href": "code/3_working_with_databases.html#working-with-sql",
    "title": "4  Working with databases",
    "section": "4.5 Working with SQL",
    "text": "4.5 Working with SQL\n\n4.5.1 Setting up BigQuery, including sandbox and billing options\nBigQuery is a data warehouse on Google Cloud that data analysts can use to query, filter large datasets, aggregate results, and perform complex operations.\nTypes of accounts:\n\nA Sandbox account is available at no charge and anyone with a Google account can log in and use it. . There are a couple of limitations to this account type. For example, you get a maximum of 12 projects at a time. It also doesn’t allow you to insert new records to a database or update the field values of existing records. Setup instructions are found here\nThe free trial gives you access to more of what BigQuery has to offer with fewer overall limitations. The free trial offers 300 dollars in credit for use in Google Cloud during the first 90 days. After you spend the $300 credit or after 90 days, your free trial will expire and you will need to personally select to upgrade to a paid account to keep working in Google Cloud.\n\nHow to get to the BigQuery console:\n\nIn your browser, go to console.cloud.google.com/bigquery taking you to the main dashboard for the Google Cloud Platform. To navigate to BigQuery from the dashboard, do the following:\n\nClick the Navigation menu icon (Hamburger icon) in the banner.\nScroll down to the BIG DATA section.\nClick BigQuery and select SQL workspace.\n\n\n\n\n4.5.2 Other databases\n\nGetting started with MySQL: This is a guide to setting up and using MySQL.\nGetting started with Microsoft SQL Server: This is a tutorial to get started using SQL Server.\nGetting started with PostgreSQL: This is a tutorial to get started using PostgreSQL.\nGetting started with SQLite: This is a quick start guide for using SQLite.\n\n\n\n4.5.3 Using BigQuery\nPlay with some public data\n\nGo to console.cloud.google.com/bigquery\nNavigate to the Explorer menu in BigQuery.\nType the word public in the search box and enter. 3 Click “Broaden search to all projects”\nFind the bigquery-public-data and pin it.\n\n\n\n4.5.4 SQL basics\nWith SQL, capitalization usually doesn’t matter. You could write SELECT or select or SeLeCT. They all work! But if you use capitalization as part of a consistent style your queries will look more professional.\nTo write SQL queries like a pro, it is always a good idea to use all caps for clause starters (e.g., SELECT, FROM, WHERE, etc.). Functions should also be in all caps (e.g., SUM()). Column names should be all lowercase (refer to the section on snake_case later in this guide). Table names should be in CamelCase (refer to the section on CamelCase later in this guide). This helps keep your queries consistent and easier to read while not impacting the data that will be pulled when you run them\nFor the most part, it also doesn’t matter if you use single quotes ’ ’ or double quotes ” ” when referring to strings.\nBut there are two situations where it does matter what kind of quotes you use:\n\nWhen you want strings to be identifiable in any SQL dialect (i.e. WHERE country = ‘US’)\nWhen your string contains an apostrophe or quotation marks (i.e. “Shepherd’s pie”)\n\nWithin each SQL dialect there are rules for what is accepted and what isn’t. But a general rule across almost all SQL dialects is to use single quotes for strings.\nIt is important to always make sure that the output of your query has easy-to-understand names. However, Never use spaces. The best practice is to use snake_case. This means that ‘total tickets’, which has a space between the two words, should be written as ‘total_tickets’ with an underscore instead of a space.\nYou can also use CamelCase capitalization when naming your table. CamelCase capitalization means that you capitalize the start of each word, like a two-humped (Bactrian) camel. So the table TicketsByOccasion uses CamelCase capitalization.\nAs a general rule, you want to keep the length of each line in a query <= 100 characters. This makes your queries easy to read.\nMulti-line comments If you make comments that take up multiple lines, you can use – for each line. Or, if you have more than two lines of comments, it might be cleaner and easier is to use /* to start the comment and */ to close the comment.\nKey terms:\n\nSELECT is the section of a query that indicates what data you want SQL to return to you\nFROM is the section of a query that indicates which table the desired data comes from.\nWHERE is the section of a query that indicates any filters you’d like to apply to your dataset\nyou’re sorting how you want your results to appear with ORDER BY.\nLIMIT tells SQL to only return the top five most popular names and their counts\n\nSELECT name, count FROM `lithe-vault-366813.babynames.names_2014` \nWHERE gender = \"M\"\nOrder BY count DESC\nLIMIT 5\nSome more resources:\n\nSearch and replace in Sublime Text\nRegex tutorial (if you don’t know what regular expressions are)\nRegex cheat sheet"
  },
  {
    "objectID": "code/4_data_orga_and_online_presence.html",
    "href": "code/4_data_orga_and_online_presence.html",
    "title": "5  Organizing data and managing your online presence",
    "section": "",
    "text": "Makes it easier to find and use data,\nHelps you avoid making mistakes during your analysis\nHelps to protect data\n\nNaming conventions: These are consistent guidelines that describe the content, date, or version of a file in its name. You want to use logical and descriptive names for your files to make them easier to find and use\nOrganizing your files into folders helps keep project-related files together in one place. This is called foldering.\nAlign your naming and storage practices and develop metadata practices with your team to avoid any confusion\n\n\nRecommendations for naming conventions:\nWhen you use consistent guidelines that describe the content, date, or version of a file and its name, you’re using file naming conventions. We can use metadata to to indicate consistent naming conventions for a project.\n\nWork out and agree on file naming conventions early on in a project to avoid renaming files again and again.\nAlign your file naming with your team’s or company’s existing file-naming conventions.\nEnsure that your file names are meaningful; consider including information like project name and anything else that will help you quickly identify (and use) the file for the right purpose.\nInclude the date and version number in file names; common formats are YYYYMMDD for dates and v## for versions (or revisions).\nCreate a text file as a sample file with content that describes (breaks down) the file naming convention and a file name that applies it.\nAvoid spaces and special characters in file names. Instead, use dashes, underscores, or capital letters. Spaces and special characters can cause errors in some applications.\nLead revision numbers with 0: v02\nCreate a text file that lays out all your naming conventions on a project. This is really helpful if someone new joins your team,"
  },
  {
    "objectID": "code/4_data_orga_and_online_presence.html#securing-data",
    "href": "code/4_data_orga_and_online_presence.html#securing-data",
    "title": "5  Organizing data and managing your online presence",
    "section": "5.2 Securing data",
    "text": "5.2 Securing data\nSecurity features can be designed to keep unauthorized users from viewing certain files, or just lock your worksheets so that you don’t accidentally break your formulas. This is called data security.\nData security means protecting data from unauthorized access or corruption by adopting safety measures.\nEncryption uses a unique algorithm to alter data and make it unusable by users and applications that don’t know the algorithm. This algorithm is saved as a “key” which can be used to reverse the encryption; so if you have the key, you can still use the data in its original form.\nTokenization replaces the data elements you want to protect with randomly generated data referred to as a “token.” The original data is stored in a separate location and mapped to the tokens. To access the complete original data, the user or application needs to have permission to use the tokenized data and the token mapping. This means that even if the tokenized data is hacked, the original data is still safe and secure in a separate location."
  },
  {
    "objectID": "code/4_data_orga_and_online_presence.html#online-presence",
    "href": "code/4_data_orga_and_online_presence.html#online-presence",
    "title": "5  Organizing data and managing your online presence",
    "section": "5.3 Online presence",
    "text": "5.3 Online presence"
  }
]