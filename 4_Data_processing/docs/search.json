[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GDA: Processing data data",
    "section": "",
    "text": "You’ll learn how to check and clean your data using spreadsheets and SQL as well as how to verify and report your data cleaning results. Current Google data analysts will continue to instruct and provide you with hands-on ways to accomplish common data analyst tasks with the best tools and resources.\nLearners who complete this certificate program will be equipped to apply for introductory-level jobs as data analysts. No previous experience is necessary.\nBy the end of this course, you will be able to do the following:\n\nLearn how to check for data integrity.\nDiscover data cleaning techniques using spreadsheets.\nDevelop basic SQL queries for use on databases.\nApply basic SQL functions for cleaning and transforming data.\nGain an understanding of how to verify the results of cleaning data.\nExplore the elements and importance of data cleaning reports."
  },
  {
    "objectID": "code/1_data_integrity.html",
    "href": "code/1_data_integrity.html",
    "title": "2  Data integrity",
    "section": "",
    "text": "Data integrity is the accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle. A good analysis depends on the integrity of the data, and data integrity usually depends on using a common format. So it is important to double-check how dates are formatted to make sure what you think is December 10, 2020 isn’t really October 12, 2020, and vice versa.\nData integrity can be compromised in lots of different ways. There’s a chance data can be compromised every time it’s replicated, transferred, or manipulated in any way. - Data replication is the process of storing data in multiple locations. If you’re replicating data at different times in different places, there’s a chance your data will be out of sync. - Data transfer is the process of copying data from a storage device to memory, or from one computer to another. - Data manipulation: The process of changing data to make it more organized and easier to read\nIt’s also important to check that the data you use aligns with the business objective."
  },
  {
    "objectID": "code/1_data_integrity.html#resources-to-check-data-integrity-in-excel",
    "href": "code/1_data_integrity.html#resources-to-check-data-integrity-in-excel",
    "title": "2  Data integrity",
    "section": "2.1 Resources to check data integrity in excel",
    "text": "2.1 Resources to check data integrity in excel\n\nVLOOKUP is a spreadsheet function that searches for a certain value in a column to return a related piece of information. Using VLOOKUP can save a lot of time; without it, you have to look up dates and names manually.\nDATEDIF function to automatically calculate the difference between the dates in column"
  },
  {
    "objectID": "code/1_data_integrity.html#dealing-with-insufficient-data",
    "href": "code/1_data_integrity.html#dealing-with-insufficient-data",
    "title": "2  Data integrity",
    "section": "2.2 Dealing with insufficient data",
    "text": "2.2 Dealing with insufficient data\nTypes of insufficient data:\n\nNo data\nTo little data\nWrong data, including data with errors\nData from only one source\nData that keeps updating (and thus is still incoming and maybe not complete)\nOutdated data\nGeographically-limited data\n\nWay to address insufficient data:\n\nGather the data on a small scale to perform a preliminary analysis and then request additional time to complete the analysis after you have collected more data\nIf there isn’t time to collect data, perform the analysis using proxy data from other datasets\nIdentify trends with available data\nWait for more data if time allows\nTalk with stakeholders and adjust objective\nLook for new dataset\nIdentify errors in the data and, if possible, correct them at the source by looking for a pattern in the errors\n\n\n2.2.1 The importance of sample size\nWhen you use sample size or a sample, you use a part of a population that’s representative of the population. The goal is to get enough information from a small group within a population to make predictions or conclusions about the whole population.\nWhen you only use a small sample of a population, it can lead to uncertainty. You can’t really be 100 percent sure that your statistics are a complete and accurate representation of the population. This leads to sampling bias, i.e. when a sample isn’t representative of the population as a whole.\nUsing random sampling can help address some of those issues with sampling bias. Random sampling is a way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen.\n\n\n2.2.2 Calculating sample size\n\nPopulation: The entire group that you are interested in for your study. For example, if you are surveying people in your company, the population would be all the employees in your company.\nSample: A subset of your population.\nMargin of error: Since a sample is used to represent a population, the sample’s results are expected to differ from what the result would have been if you had surveyed the entire population. This difference is called the margin of error. The smaller the margin of error, the closer the results of the sample are to what the result would have been if you had surveyed the entire population.\nConfidence level: How confident you are in the survey results or the probability that your sample accurately reflects the greater population. For example, a 95% confidence level means that if you were to run the same survey 100 times, you would get similar results 95 of those 100 times. Confidence level is targeted before you start your study because it will affect how big your margin of error is at the end of your study. Having a 99 percent confidence level is ideal. But most industries hope for at least a 90 or 95 percent confidence level.\nConfidence interval: The range of possible values that the population’s result would be at the confidence level of the study. This range is the sample result +/- the margin of error.\nStatistical significance: The determination of whether your result could be due to random chance or not. The greater the significance, the less due to chance.\n\nBasic rules:\n\nDon’t use a sample size less than 30. It has been statistically proven that 30 is the smallest sample size where an average result of a sample starts to represent the average result of a population. This recommendation is based on the Central Limit Theorem (CLT). As sample size increases, the results more closely resemble the normal (bell-shaped) distribution from a large number of samples. A sample of 30 is the smallest sample size for which the CLT is still valid.\nThe confidence level most commonly used is 95%, but 90% can work in some cases.\nFor a higher confidence level, use a larger sample size\nTo decrease the margin of error, use a larger sample size\nFor greater statistical significance, use a larger sample size\n\nRefer to the Determine the Best Sample Size video for a demonstration of a sample size calculator, or refer to the Sample Size Calculator reading for additional information."
  },
  {
    "objectID": "code/1_data_integrity.html#testing-your-data",
    "href": "code/1_data_integrity.html#testing-your-data",
    "title": "2  Data integrity",
    "section": "2.3 Testing your data",
    "text": "2.3 Testing your data\n\n2.3.1 Statistical power\n\nStatistical power is the probability of getting meaningful results from a test\nHypothesis testing is a way to see if a survey or experiment has meaningful results\nIf a test is statistically significant, it means the results of the test are real and not an error caused by random chance. Usually, you need a statistical power of at least 0.8 or 80% to consider your results statistically significant\n\nRead more here\n\n\n2.3.2 Determine the best sample size\nA sample size calculator tells you how many people you need to interview (or things you need to test) to get results that represent the target population. See the tutorials folder for an example of a calculator for sample size. To other examples:\n\nSample size calculator by surveymonkey.com\nSample size calculator by raosoft.com\n\nAfter you have plugged your information into one of these calculators, it will give you a recommended sample size. Keep in mind, the calculated sample size is the minimum number to achieve what you input for confidence level and margin of error.\nIf you are working with a survey, you will also need to think about the estimated response rate to figure out how many surveys you will need to send out. For example, if you need a sample size of 100 individuals and your estimated response rate is 10%, you will need to send your survey to 1,000 individuals to get the 100 responses you need for your analysis.\n\n\n2.3.3 Consider the margin of error\nMargin of error is the maximum amount that the sample results are expected to differ from those of the actual population. More technically, the margin of error defines a range of values below and above the average result for the sample.\nMargin of error helps you understand how reliable the data from your hypothesis testing is.\nThe closer to zero the margin of error, the closer your results from your sample would match results from the overall population.\nTo calculate the margin of error, we need:\n\nPopulation size\nSample size\nConfidence level\n\nSome online calculators:\n\nMargin of error calculator by G​ood Calculators (free online calculators)\nMargin of error calculator by CheckMarket"
  },
  {
    "objectID": "code/2_data_cleaning.html",
    "href": "code/2_data_cleaning.html",
    "title": "3  Data cleaning",
    "section": "",
    "text": "The problem with data cleaning is that it usually requires a lot of time, energy, and attention from a junior data analyst. One of the best ways to lessen the negative impacts of data cleaning is to have a plan of action or a specific approach to cleaning the data."
  },
  {
    "objectID": "code/2_data_cleaning.html#basics",
    "href": "code/2_data_cleaning.html#basics",
    "title": "3  Data cleaning",
    "section": "3.1 Basics",
    "text": "3.1 Basics\nDirty data is data that’s incomplete, incorrect, or irrelevant to the problem you’re trying to solve.\nData engineers transform data into a useful format for analysis and give it a reliable infrastructure. This means they develop, maintain, and test databases, data processors and related systems.\nData warehousing specialists develop processes and procedures to effectively store and organize data. They make sure that data is available, secure, and backed up to prevent loss.\nA null is an indication that a value does not exist in a data set. Note that it’s not the same as a zero. In the case of a survey, a null would mean the customers skipped that question. A zero would mean they provided zero as their response."
  },
  {
    "objectID": "code/2_data_cleaning.html#recognzing-and-remedying-dirty-data",
    "href": "code/2_data_cleaning.html#recognzing-and-remedying-dirty-data",
    "title": "3  Data cleaning",
    "section": "3.2 Recognzing and remedying dirty data",
    "text": "3.2 Recognzing and remedying dirty data\nTypes of dirty data:\n\nDuplicate data\nOutdated data\nIncomplete data\nIncorrect/inaccurate data\nInconsistent data or formatting\nMissing data or nulls\nDifferent currencies, different date formats, etc\nInconsistent field length, i.e. field length is a tool for determining how many characters can be keyed into a field. his is part of data validation. Data validation is a tool for checking the accuracy and quality of data before adding or importing it."
  },
  {
    "objectID": "code/2_data_cleaning.html#cleaning-data",
    "href": "code/2_data_cleaning.html#cleaning-data",
    "title": "3  Data cleaning",
    "section": "3.3 Cleaning data",
    "text": "3.3 Cleaning data\nSome of the errors you might come across while cleaning your data could include:\n\nNot checking spelling errors\nForgetting to document errors\nNot checking for misfielded values, i.e. a misfielded value happens when the values are entered into the wrong field\nOverlooking missing values\nLooking at a subset of the data and not the full picture\nLoosing track of business objectives\nNot fixing the source of the error\nNot analyzing the system prior to cleaning\nNot backing up your data prior to cleaning\nNot accounting for data cleaning in your deadline/process\n\nData merging is the process of combining two or more datasets into a single dataset. This presents a unique challenge because when two totally different datasets are combined, the information is almost guaranteed to be inconsistent and misaligned.\nCompatibility describes how well two or more datasets are able to work together. Questions to ask: - Do I have all the data I need? - Does the data I need exists within these datasets? - Do the datasets need to be cleaned or are they ready for me to use? - Are the datasets cleaned to the same standard?\nResources:\n\nTop ten ways to clean your data: Review an orderly guide to data cleaning in Microsoft Excel.\n10 Google Workspace tips to clean up data: Learn best practices for data cleaning in Google Sheets."
  },
  {
    "objectID": "code/2_data_cleaning.html#data-cleaning-in-spreadsheets",
    "href": "code/2_data_cleaning.html#data-cleaning-in-spreadsheets",
    "title": "3  Data cleaning",
    "section": "3.4 Data cleaning in spreadsheets",
    "text": "3.4 Data cleaning in spreadsheets\n\n3.4.1 Conditional formatting\nConditional formatting is a spreadsheet tool that changes how cells appear when values meet (or don’t meet) specific conditions. We can for example use it to highlight blank cells.\nFirst, we select the columns we want to work on.\nTo select non-adjacent rows or columns, hold Ctrl and select the row or column numbers (in my case the windows button). I.e. we can first select all cells and then press control and i.e. columns F and H if we want to unselect them.\nGo to format –> conditional formatting –> format rules –> is empty –> change formatting style to pink\n\n\n3.4.2 Removing duplicates\nGo to data –> data cleanup –> remove duplicates\n\n\n3.4.3 Making formats consistent\nI.e. sometimes date formats are not consistent, to clean this:\nSelect relevant column –> Format –> Number –> Date\n\n\n3.4.4 Cleaning text strings\nA text string is a group of characters within a cell, most often composed of letters. An important characteristic of a text string is its length, which is the number of characters in it.\nSplit is a tool that divides a text string around the specified character and puts each fragment into a new and separate cell. Split is helpful when you have more than one piece of data in a cell and you want to separate them out, i.e. first and last name. To do this:\nSelect column –> Data –> Split text to column (in GoogleSheets, delimiter is automaticall detected)\nDealing with text that actually is a number:\nSelect column –> Data –> Split text to columns\n\n\n3.4.5 Trim white space\nIn Excel, you can use the TRIM command to get rid of white spaces. In any space beneath your data (such as cell A10), type =TRIM(A1). Then, drag the bottom right corner of the cell to the bottom right to call the data without the white spaces.\n\n\n3.4.6 Change Text Lower/Uppercase/Proper Case\nIf you are using Microsoft Excel, this documentation explains how to use a formula to change the case of a text string. Follow these instructions to clean the string text and then move on to the confirmation and reflection section of this activity.\nIf you’re completing this exercise using Google Sheets, you’ll need to install an add-in that will give you the functionality needed to easily clean string data and change cases.\nGoogle Sheets Add-on Instructions:\n\nClick on the Add-Ons option at the top of Google Sheets.\nClick on Get add-ons.\nSearch for ChangeCase.\nSelect the column you want to edit\nClick on the Add-Ons tab and select ChangeCase. Select the option All uppercase. Notice the other options that you could have chosen if needed.\n\n\n\n3.4.7 Delete formatting\nf you want to clear the formatting for any or all cells, you can find the command in the Format tab. To clear formatting:\n\nSelect the data for which you want to delete the formatting. In this case, highlight all the data in the spreadsheet by clicking and dragging over Rows 1-8.\nClick the Format tab and select the Clear Formatting option.\n\nIn Excel, go to the Home tab, then hover over Clear and select Clear Formats.\nYou will notice that all the cells have had their formatting removed."
  },
  {
    "objectID": "code/2_data_cleaning.html#optimizing-the-data-cleaning-process",
    "href": "code/2_data_cleaning.html#optimizing-the-data-cleaning-process",
    "title": "3  Data cleaning",
    "section": "3.5 Optimizing the data cleaning process",
    "text": "3.5 Optimizing the data cleaning process\n\n3.5.1 Countif\nCOUNTIF is a function that returns the number of cells that match a specified value. Basically, it counts the number of times a value appears in a range of cells.\nEvery function has a certain syntax that needs to be followed for it to work. Syntax is a predetermined structure that includes all required information and its proper placement.\n=COUNTIF(range, \"value\") for example: =COUNTIF(range, \"<100\")\n\n\n3.5.2 Len\nLEN is a function that tells you the length of the text string by counting the number of characters it contains. This is useful when cleaning data if you have a certain piece of information in your spreadsheet that you know must contain a certain length.\nLEN(range)\n\n\n3.5.3 LEFT and RIGHT\nLEFT is a function that gives you a set number of characters from the left side of a text string. RIGHT is a function that gives you a set number of characters from the right side of a text string.\nLEFT(range, number of characters)\n\n\n3.5.4 MID\nMID is a function that gives you a segment from the middle of a text string.\nMID(range, reference starting point, number of middle characters)\n\n\n3.5.5 Concatenate\nCONCATENATE, which is a function that joins together two or more text strings.\n=CONCATENATE(item1, item2)\n\n\n3.5.6 SPLIT\nThe SPLIT function divides text around a specified character or string, and puts each fragment of text into a separate cell in the row.\n=SPLOT(cell, \"delimiter\")\n\n\n3.5.7 TRIM\nTRIM is a function that removes leading, trailing, and repeated spaces in data. Sometimes when you import data, your cells have extra spaces, which can get in the way of your analysis.\n=TRIM(range)"
  },
  {
    "objectID": "code/2_data_cleaning.html#workflow-automatation",
    "href": "code/2_data_cleaning.html#workflow-automatation",
    "title": "3  Data cleaning",
    "section": "3.6 Workflow automatation",
    "text": "3.6 Workflow automatation\nBasically, workflow automation is the process of automating parts of your work.\n\nTowards Data Science’s Automating Scientific Data Analysis\nMIT News’ Automating Big-Data Analysis\nTechnologyAdvice’s 10 of the Best Options for Workflow Automation Software"
  },
  {
    "objectID": "code/2_data_cleaning.html#different-data-perspectives",
    "href": "code/2_data_cleaning.html#different-data-perspectives",
    "title": "3  Data cleaning",
    "section": "3.7 Different data perspectives",
    "text": "3.7 Different data perspectives\n\n3.7.1 Pivot table\nSelect the data we want to use –> Insert –> Pivot table –> Add row (i.e. total profits), you can also add another row, i.e. products\n\n\n3.7.2 VLOOKUP\nVLOOKUP stands for vertical lookup. It’s a function that searches for a certain value in a column to return a corresponding piece of information. When data analysts look up information for a project, it’s rare for all of the data they need to be in the same place. Usually, you’ll have to search across multiple sheets or even different databases.\n=VLOOKUP(data to look up, 'where to look'!Range, column, false)\nIn our example, this will be the name of a spreadsheet followed by an exclamation point. The exclamation point indicates that we’re referencing a cell in a different sheet from the one we’re currently working in.\nThe column in the range containing the value to return.\nThe word “false,” means that an exact match is what we’re looking for."
  },
  {
    "objectID": "code/2_data_cleaning.html#data-mapping",
    "href": "code/2_data_cleaning.html#data-mapping",
    "title": "3  Data cleaning",
    "section": "3.8 Data mapping",
    "text": "3.8 Data mapping\nData mapping is the process of matching fields from one database to another. This is very important to the success of data migration, data integration, and lots of other data management activities.\nThe first step to data mapping is identifying what data needs to be moved. This includes the tables and the fields within them. We also need to define the desired format for the data once it reaches its destination."
  },
  {
    "objectID": "code/3_sql_and_data_cleaning.html",
    "href": "code/3_sql_and_data_cleaning.html",
    "title": "4  Using SQL to clean data",
    "section": "",
    "text": "Spreadsheets: - Generated with a program - Access to the data you input - Stored locally - Smaller datasets - Work on it independently - Build-in functionalities\nSQL: - A language used to interact with database programs - Can pull information from different sources in the database - Stored across a database - Larger datasets - Track changes across a team - Useful across multiple programs\nSome database products have their own variant of SQL, and these different varieties of SQL dialects are what help you communicate with each database product."
  },
  {
    "objectID": "code/3_sql_and_data_cleaning.html#widely-used-sql-queries",
    "href": "code/3_sql_and_data_cleaning.html#widely-used-sql-queries",
    "title": "4  Using SQL to clean data",
    "section": "4.1 Widely used SQL queries",
    "text": "4.1 Widely used SQL queries\nQueries were run using BigQuery after uploading the file in data/customer_data.csv as well as data/cars.csv to the sandbox\n\n4.1.1 Select\nSELECT and FROM help specify what data we want to extract from the database and use.\nSELECT name, city\nFROM `lithe-vault-366813.customer_data.customer_address` \nLIMIT 5\n\n\n4.1.2 Insert data into databases\nNotice, with the BigQuery sandbox account, you won’t be able to use INSERT INTO or UPDATE\nWe add new data like this:\n\nDefine into which table we want to add data\nSpecify which columns we’re adding this data to by typing their names in the parentheses\nSay what we want to add\n\nINSERT INTO `lithe-vault-366813.customer_data.customer_address`\n    (customer_id, adress, city, state, zipcode, country)\nVALUES \n    (2645, '333 SQL Road', 'Jackson', 'MI', 49202, 'US')\nWe change data like this:\n\nDefine what table we want to update\nLet it know what value we’re trying to change\nTell SQL where we want to make the change\n\nUPDATE `lithe-vault-366813.customer_data.customer_address`\nSET address = '123 New Address'\nWHERE customer_id = 2645\n\n\n4.1.3 Creating a new table\nIf we want to create a new table for this database, we can use the CREATE TABLE IF NOT EXISTS statement.\nKeep in mind, just running a SQL query doesn’t actually create a table for the data we extract. It just stores it in our local memory. To save it, we’ll need to download it as a spreadsheet or save the result into a new table.\nAnother good thing to keep in mind, if you’re creating lots of tables within a database, you’ll want to use the DROP TABLE IF EXISTS statement to clean up after yourself."
  },
  {
    "objectID": "code/3_sql_and_data_cleaning.html#cleaning-string-variables-using-sql",
    "href": "code/3_sql_and_data_cleaning.html#cleaning-string-variables-using-sql",
    "title": "4  Using SQL to clean data",
    "section": "4.2 Cleaning string variables using SQL",
    "text": "4.2 Cleaning string variables using SQL\n\n4.2.1 Removing duplicates\nEarlier, we covered how to remove duplicates in spreadsheets using the Remove duplicates tool. In SQL, we can do the same thing by including DISTINCT in our SELECT statement.\nSELECT DISTINCT customer_id\nFROM `lithe-vault-366813.customer_data.customer_address` \nLIMIT 20\n\n\n4.2.2 LENGTH\nIf we already know the length our string variables are supposed to be, we can use LENGTH to double-check that our string variables are consistent. For some databases, this query is written as LEN, but it does the same thing.\nTo remind ourselves what our results mean, we can label this column in our results as letters_in_country.\nSELECT LENGTH(country) AS letters_in_country\nFROM `lithe-vault-366813.customer_data.customer_address` \nIt seems like almost all of them are 2s, which means the country field contains only two letters. But let’s check out if countries are incorrectly listed in our table.\nSELECT country\nFROM `lithe-vault-366813.customer_data.customer_address` \nWHERE LENGTH(country) > 2\nThe incorrectly listed countries show up as USA instead of US\nWe still need to fix this problem so we can pull a list of all the customers in the US, including the two that have USA instead of US. The good news is that we can account for this error in our results by using the substring function in our SQL query.\nWe’re going to use the substring function to pull the first two letters of each country so that all of them are consistent and only contain two letters.\nTo use the substring function, we first need to tell SQL the column where we found this error, country. Then we specify which letter to start with.Then we specify which letter to start with. We want SQL to pull the first two letters, so we’re starting with the first letter, so we type in 1. Then we need to tell SQL how many letters, including this first letter, to pull. Since we want the first two letters, we need SQL to pull two total letters, so we type in 2.\nSELECT customer_id\nFROM `lithe-vault-366813.customer_data.customer_address` \nWHERE SUBSTR(country, 1, 2) = 'US'\nWhen we run this query, we get a list of all customer IDs of customers whose country is the US, including the customers that had USA instead of US.\n\n\n4.2.3 TRIM\nThis is really useful if you find entries with extra spaces and need to eliminate those extra spaces for consistency.\nWe first want SQL to filter for states that have more than two letters:\nSELECT  state\nFROM `lithe-vault-366813.customer_data.customer_address` \nWHERE LENGTH(state) > 2\nWe have one state that has more than two letters. But hold on, how can this state that seems like it has two letters, O and H for Ohio, have more than two letters? The extra characters must be a space\nLets deal with this:\nSELECT  DISTINCT customer_id\nFROM `lithe-vault-366813.customer_data.customer_address` \nWHERE TRIM(state) = 'OH'\n\n\n4.2.4 Find min and max values\nIn another dataframe, we inspect car measurements. The length column should contain numeric measurements of the cars. So you will check that the minimum and maximum lengths in the dataset align with the data description, which states that the lengths in this column should range from 141.1 to 208.1.\nSELECT\n  MIN(length) AS min_length,\n  MAX(length) AS max_length\nFROM `lithe-vault-366813.cars.car_info` \n\n\n4.2.5 Missing data\nMissing values can create errors or skew your results during analysis. You’re going to want to check your data for null or missing values. These values might appear as a blank cell or the word null in BigQuery.\nWe find such instances with:\nSELECT *\nFROM `lithe-vault-366813.cars.car_info` \nWHERE num_of_doors IS NULL\nThis does not work in the sandbox, but one way to update these values is\nUPDATE cars.car_info\nSET num_of_doors = \"four\"\nWHERE\n  make = \"dodge\"\n  AND fuel_type = \"gas\"\n  AND body_style = \"sedan\";\n\n\n4.2.6 Correct misspellings\nImagine some numbers write tow instead of two, we can correct this with:\nUPDATE cars.car_info\nSET num_of_cylinders = \"two\"\nWHERE num_of_cylinders = \"tow\";\n\n\n4.2.7 Check how many rows contain a specific piece of info\nWe have measurements of 70, which is too high. Before you delete anything, you should check to see how many rows contain this erroneous value as a precaution so that you don’t end up deleting 50% of your data. If there are too many (for instance, 20% of your rows have the incorrect 70 value), then you would want to check back in with the sales manager to inquire if these should be deleted or if the 70 should be updated to another value. Use the query below to count how many rows you would be deleting:\nSELECT COUNT(*) AS num_rows_to_delete\nFROM `lithe-vault-366813.cars.car_info` \nWHERE compression_ratio = 70;\n\n\n4.2.8 Converting between data types\n\n4.2.8.1 Strings and numbers\nCAST can be used to convert anything from one data type to another.\nLet’s start by trying to the different purchase prices. For this, we loaded the table data/Furniture-Store-Transaction-Table.csv into BigQuery\nSELECT purchase_price  \nFROM `lithe-vault-366813.customer_data.customer_purchase` \nORDER BY purchase_price DESC\nWhen we do this we see there is sth wrong with the order:\n1: 89.85\n2: 799.99\n3: 58.89\nThe problem is that the database doesn’t recognize that these are numbers, so it didn’t sort them that way. If we go back to the customer_purchase table and take a look at its schema, we can see what datatype that database thinks purchase underscore price is a string while it is a float, i.e. a number that contains a decimal.\nTypecasting means converting data from one type to another, which is what we’ll do with the CAST function.\nSELECT CAST(purchase_price AS FLOAT64)  \nFROM `lithe-vault-366813.customer_data.customer_purchase` \nORDER BY CAST(purchase_price AS FLOAT64) DESC\n\n\n4.2.8.2 Dates\nThe furniture store owner has asked us to look at purchases that occurred during their sales promotion period in December.\nSELECT date, purchase_price \nFROM `lithe-vault-366813.customer_data.customer_purchase` \nWHERE date BETWEEN '2020-12-01' AND '2020-12-31'\nFour purchases occurred in December, but the date field looks odd. That’s because the database recognizes this date field as datetime, which consists of the date and time. Our SQL query still works correctly, even if the date field is datetime instead of date. But we can tell SQL to convert the date field into the date data type so we see just the day and not the time.\nSELECT CAST(date AS date) AS date_only,\n  purchase_price\nFROM `lithe-vault-366813.customer_data.customer_purchase` \nWHERE date BETWEEN '2020-12-01' AND '2020-12-31'\n\n\n\n4.2.9 Combining strings\nCONCAT lets you add strings together to create new text strings that can be used as unique keys.\nThe owner wants to know if customers prefer certain colors, so the owner can manage store inventory accordingly. The problem is, the product_code is the same, regardless of the product color. We need to find another way to separate products by color.\nSELECT CONCAT(product_code, product_color) AS new_product_code\nFROM `lithe-vault-366813.customer_data.customer_purchase` \nWHERE product = 'couch'\n\n\n4.2.10 Identify non-null values\nCOALESCE can be used to return non-null values in a list. Null values are missing values. If you have a field that’s optional in your table, it’ll have null in that field for rows that don’t have appropriate values to put there. I.e. we have some rows with missing product information.\nWe want to use the product_name column to understand what kind of product was sold. We want a list of product names, but if names aren’t available, then give us the product code. Here is where we type “COALESCE.” then we tell SQL which column to check first, product, and which column to check second if the first column is null, product_code.\nSELECT DISTINCT COALESCE(product, product_code) AS product_info\nFROM `lithe-vault-366813.customer_data.customer_purchase`"
  },
  {
    "objectID": "code/4_verifying_and_reporting.html",
    "href": "code/4_verifying_and_reporting.html",
    "title": "5  Verifying and reporting results",
    "section": "",
    "text": "Verification is a process to confirm that a data cleaning effort was well- executed and the resulting data is accurate and reliable. It involves rechecking your clean dataset, doing some manual clean ups if needed, and taking a moment to sit back and really think about the original purpose of the project.\nThe first step in verification is returning to your original, unclean dataset and comparing it to what you have now.\nThe most common problems are:\n\nSources of errors: Did you use the right tools and functions to find the source of the errors in your dataset?\nNull data: Did you search for NULLs using conditional formatting and filters?\nMisspelled words: Did you locate all misspellings?\nMistyped numbers: Did you double-check that your numeric data has been entered correctly?\nExtra spaces and characters: Did you remove any extra spaces or characters using the TRIM function?\nDuplicates: Did you remove duplicates in spreadsheets using the Remove Duplicates function or DISTINCT in SQL?\nMismatched data types: Did you check that numeric, date, and string data are typecast correctly?\nMessy (inconsistent) strings: Did you make sure that all of your strings are consistent and meaningful?\nMessy (inconsistent) date formats: Did you format the dates consistently throughout your dataset?\nMisleading variable labels (columns): Did you name your columns meaningfully?\nTruncated data: Did you check for truncated or missing data that needs correction?\nBusiness Logic: Did you check that the data makes sense given your knowledge of the business?\n\nSeeing the big picture:\n\nConsider the business problem to know what data you need\nConsider the goal of your project and if the data helps achieve that goal\nConsider the data, i.e. is it capable of meeting the goals; asking whether the data makes sense\n\n\n\nCOUNTA counts the total number of values within a specified range. Note that there’s also function called COUNT, which only counts the numerical values within a specified range.\nIf you’re working in SQL, you can address misspellings using a CASE statement. The CASE statement goes through one or more conditions and returns a value as soon as a condition is met. I.e. in the example below, a customers first name was misspelled\nSELECT customer_id, \n    CASE\n        WHEN first_name = 'Tnoy' THEN 'Tony'\n        WHEN first_name = 'Tmo' THEN 'Tom'\n        ELSE first_name\n        END AS cleaned_name\nFROM customer_data.customer_name"
  },
  {
    "objectID": "code/4_verifying_and_reporting.html#documenting-results-and-the-cleaning-process",
    "href": "code/4_verifying_and_reporting.html#documenting-results-and-the-cleaning-process",
    "title": "5  Verifying and reporting results",
    "section": "5.2 Documenting results and the cleaning process",
    "text": "5.2 Documenting results and the cleaning process\nDocumentation which is the process of tracking changes, additions, deletions and errors involved in your data cleaning effort.\nHaving a record of how a data set evolved does three very important things:\n\nRecover data-cleaning errors\nInform other users of changes\nDetermine the quality of the data\n\nA changelog is a file containing a chronologically ordered list of modifications made to a project. It’s usually organized by version and includes the date followed by a list of added, improved, and removed features.\nTypically, a changelog records this type of information:\n\nData, file, formula, query, or any other component that changed\nDescription of what changed\nDate of the change\nPerson who made the change\nPerson who approved the change\nVersion number\nReason for the change\n\nCategories of changes:\n\nAdded: new features introduced\nChanged: changes in existing functionality\nDeprecated: features about to be removed\nRemoved: features that have been removed\nFixed: bug fixes\nSecurity: lowering vulnerabilities\n\nGeneral rules:\n\nChangelogs are for humans, not machines, so write legibly.\nEvery version should have its own entry.\nEach change should have its own line.\nGroup the same types of changes. For example, Fixed should be grouped separately from Added.\nVersions should be ordered chronologically starting with the latest.\nThe release date of each version should be noted.\n\nLet’s start with the spreadsheet. We can use Sheet’s version history, which provides a real-time tracker of all the changes and who made them from individual cells to the entire worksheet. To find this feature, click the File tab, and then select Version history. If you want to check out changes in a specific cell, we can right-click and select Show Edit History.\nThe way you create and view a changelog with SQL depends on the software program you’re using. Essentially, all you have to do is specify exactly what you did and why when you commit a query to the repository as a new and improved query. Another option is to just add comments as you go while you’re cleaning data in SQL. Query history, which tracks all the queries you’ve run in BigQuery. If an analyst is making changes to an existing SQL query that is shared across the company, the company most likely uses what is called a version control system.\nHere is how a version control system affects a change to a query:\n\nA company has official versions of important queries in their version control system.\nAn analyst makes sure the most up-to-date version of the query is the one they will change. This is called syncing\nThe analyst makes a change to the query.\nThe analyst might ask someone to review this change. This is called a code review and can be informally or formally done. An informal review could be as simple as asking a senior analyst to take a look at the change.\nAfter a reviewer approves the change, the analyst submits the updated version of the query to a repository in the company’s version control system. This is called a code commit. A best practice is to document exactly what the change was and why it was made in a comments area. Going back to our example of a query that pulls daily revenue, a comment might be: Updated revenue to include revenue coming from the new product, Calypso.\nAfter the change is submitted, everyone else in the company will be able to access and use this new query when they sync to the most up-to-date queries stored in the version control system.\nIf the query has a problem or business needs change, the analyst can undo the change to the query using the version control system. The analyst can look at a chronological list of all changes made to the query and who made each change. Then, after finding their own change, the analyst can revert to the previous version.\nThe query is back to what it was before the analyst made the change. And everyone at the company sees this reverted, original query, too."
  },
  {
    "objectID": "code/4_verifying_and_reporting.html#advanced-functions-for-speedy-data-cleaning",
    "href": "code/4_verifying_and_reporting.html#advanced-functions-for-speedy-data-cleaning",
    "title": "5  Verifying and reporting results",
    "section": "5.3 Advanced functions for speedy data cleaning",
    "text": "5.3 Advanced functions for speedy data cleaning\n\n5.3.1 Insert data\nThe IMPORTRANGE function in Google Sheets and the Paste Link feature (a Paste Special option in Microsoft Excel) both allow you to insert data from one sheet to another. Using these on a large amount of data is more efficient than manual copying and pasting.\n\n\n5.3.2 Pulling data from other data sources\nThe QUERY function is also useful when you want to pull data from another spreadsheet. The QUERY function’s SQL-like ability can extract specific data within a spreadsheet. For a large amount of data, using the QUERY function is faster than filtering data manually."
  },
  {
    "objectID": "code/5_hiring_process.html",
    "href": "code/5_hiring_process.html",
    "title": "6  About the data-analyst hiring process",
    "section": "",
    "text": "If you are transitioning from a different career and don’t yet have relevant work experience, then you may want to pick a format that highlights your technical skills and portfolio projects. Some resume formats include a Summary or Goals section at the top to help candidates add context to their application, while other resume formats avoid these sections completely and save that space for sections such as Skills and Experience.\nWhatever format you pick, make sure to follow the one-page rule and keep the completed version on just a single page.\nThe summary section might be a good spot to point out if you’re transitioning into a new career role.\nFocus on your accomplishments first, and explain them using the formula “Accomplished X, as measured by Y, by doing Z.” Phrase your work experience and duties using Problem-Action-Result (PAR) statements.\n\n\nExample skills to add:\n\nStrong analytical skills\nPattern recognition\nRelational databases and SQL\nStrong data visualization skills\nProficiency with spreadsheets, SQL, R, and Tableau\n\nAdd a section for programming languages\n\n\n\n\nPresentation skills\nCollaboration\nCommunication\nResearch\nProblem-solving skills\nAdaptability\nAttention to detail"
  },
  {
    "objectID": "code/5_hiring_process.html#kaggles-careercon-resources",
    "href": "code/5_hiring_process.html#kaggles-careercon-resources",
    "title": "6  About the data-analyst hiring process",
    "section": "6.2 Kaggle’s CareerCon resources",
    "text": "6.2 Kaggle’s CareerCon resources\nKaggle’s CareerCon is an annual and free digital event whose aim is to help new data analysts land their first job in the field. Browse the full sessions for CareerCon 2019 and check out Portfolio and resume analysis with data science hiring managers: A panel of hiring managers discusses what they are seeking in candidates and how they examine different resumes submitted by job seekers like you.\nOther resources:\n\nHow to build a compelling data science portfolio and resume\nOverview of the Data Science Interview Process\nLive Breakdown of Common Data Science Interview Questions\nAm I a Good Fit? Identifying Your Best Data Science Job Opportunities\nReal Stories from a Panel of Successful Career Switchers"
  }
]